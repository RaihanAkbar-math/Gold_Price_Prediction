data = golfpricefull;
trainsize = 0.8*len(golfpricefull);
datatrain = data(1:trainsize,:);
datatest  = data(trainsize+1:end,:);

%%

xtrain = table2array(datatrain(:,3:4));
ytrain = table2array(datatrain(:,2));
yfull = table2array(data(:,2));

xtest = table2array(datatest(:,3:4));
ytest = table2array(datatest(:,2));

figure('Color', 'w', 'Position', [100 100 1000 500]);
hold on
% Ekstrak harga dan waktu
harga = data.BMRI;
tanggal = data.Date;

% Indeks pembagian data
trainInd = 1:84;
valInd = 85:132;
testInd = 133:155;
% Plot masing-masing periode
xline(tanggal(85), '--k', 'Validation Start', 'LabelVerticalAlignment','bottom', 'LabelHorizontalAlignment','right');
xline(tanggal(133), '--k', 'Testing Start', 'LabelVerticalAlignment','bottom', 'LabelHorizontalAlignment','right');

%%
% Solve an Autoregression Time-Series Problem with a NAR Neural Network
% Script generated by Neural Time Series app
% Created 12-May-2025 00:42:33
%
% This script assumes this variable is defined:
%
%   ytrain - feedback time series.
for dd = 2:9
    for hh = 5:25
    rng(1)
    T = tonndata(yfull,false,false);

% Choose a Training Function
% For a list of all training functions type: help nntrain
% 'trainlm' is usually fastest.
% 'trainbr' takes longer but may be better for challenging problems.
% 'trainscg' uses less memory. Suitable in low memory situations.
trainFcn = 'trainbr';  % Levenberg-Marquardt backpropagation.

% Create a Nonlinear Autoregressive Network
feedbackDelays = 1:dd;
hiddenLayerSize = hh;
net = narnet(feedbackDelays,hiddenLayerSize,'open',trainFcn);
net.layers{1}.transferFcn = 'tansig';
fprintf('Model: %g-%g, ',dd,hh)

% Prepare the Data for Training and Simulation
% The function PREPARETS prepares timeseries data for a particular network,
% shifting time by the minimum amount to fill input states and layer
% states. Using PREPARETS allows you to keep your original time series data
% unchanged, while easily customizing it for networks with differing
% numbers of delays, with open loop or closed loop feedback modes.
[x,xi,ai,t] = preparets(net,{},{},T);

% Setup Division of Data for Training, Validation, Testing
net.divideFcn = 'divideind';
net.divideParam.trainInd = 1:84;
net.divideParam.valInd = 85:132;
net.divideParam.testInd = 133:155;

net.trainParam.showWindow = false;   % Nonaktifkan training GUI

% Train the Network
[net,tr] = train(net,x,t,xi,ai);
%
% Test the Network
y = net(x,xi,ai);
e = gsubtract(t,y);
prf = perform(net,t,y);
fprintf('perf: %.2f / ',sqrt(prf))

loopPrf(dd,hh) = prf;

% View the Network
% view(net)
% print(gcf, 'fig1', '-dpng');  % simpan diagram sebagai PNG

% Plots
% Uncomment these lines to enable various plots.
%figure, plotperform(tr)
%figure, plottrainstate(tr)
%figure, ploterrhist(e)
%figure, plotregression(t,y)
%figure, plotresponse(t,y)
%figure, ploterrcorr(e)
%figure, plotinerrcorr(x,e)

% Closed Loop Network
% Use this network to do multi-step prediction.
% The function CLOSELOOP replaces the feedback input with a direct
% connection from the output layer.
netc = closeloop(net);
netc.name = [net.name ' - Closed Loop'];
% view(netc)
[xc,xic,aic,tc] = preparets(netc,{},{},T);
yc = netc(xc,xic,aic);
closedLoopPerformance = perform(net,tc,yc);
fprintf('%.2f, ',sqrt(closedLoopPerformance))

% Step-Ahead Prediction Network
% For some applications it helps to get the prediction a timestep early.
% The original network returns predicted y(t+1) at the same time it is
% given y(t+1). For some applications such as decision making, it would
% help to have predicted y(t+1) once y(t) is available, but before the
% actual y(t+1) occurs. The network can be made to return its output a
% timestep early by removing one delay so that its minimal tap delay is now
% 0 instead of 1. The new network returns the same outputs as the original
% network, but outputs are shifted left one timestep.
nets = removedelay(net);
nets.name = [net.name ' - Predict One Step Ahead'];
% view(nets)
[xs,xis,ais,ts] = preparets(nets,{},{},T);
ys = nets(xs,xis,ais);
stepAheadPerformance = perform(nets,ts,ys);
% fprintf('Performance stepAheadPerformance = %.2f\n',sqrt(stepAheadPerformance))

xx = 1:height(data);
tt = data.Date;
xxa = xx; xxa(feedbackDelays) = [];
tta = tt; tta(feedbackDelays) = [];

figure(1)
plot(tta,[y{:}],'Color',[1, 0, 0, 0.2])
hold on
plot(tt, yfull,'k','LineWidth',2)
legend('Forecast','Actual')
yact = yfull(xxa);
ylim([0,10000])
n = net.divideParam.trainInd;
n(feedbackDelays) = [];
n = n - feedbackDelays(end);
fprintf('MAPE: -> %.2f%% | ',mape([y{n}]',yact(n)))
n = net.divideParam.valInd;
n = n - feedbackDelays(end);
fprintf('%.2f%% | ',mape([y{n}]',yact(n)))
n = net.divideParam.testInd;
n = n - feedbackDelays(end);
fprintf('%.2f%%\n',mape([y{n}]',yact(n)))

    end
end

